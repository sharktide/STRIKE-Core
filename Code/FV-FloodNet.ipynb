{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acaab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced dataset size: 2000 | Flood: 1000 | No Flood: 1000\n",
      "Epoch 1/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9954 - loss: 0.2282 - val_accuracy: 1.0000 - val_loss: 0.1240\n",
      "Epoch 2/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9998 - loss: 0.1300 - val_accuracy: 1.0000 - val_loss: 0.1219\n",
      "Epoch 3/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.1594 - val_accuracy: 1.0000 - val_loss: 0.1198\n",
      "Epoch 4/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.1282 - val_accuracy: 1.0000 - val_loss: 0.1196\n",
      "Epoch 5/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9997 - loss: 0.1252 - val_accuracy: 1.0000 - val_loss: 0.1255\n",
      "Epoch 6/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9875 - loss: 0.1467 - val_accuracy: 1.0000 - val_loss: 0.1182\n",
      "Epoch 7/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9982 - loss: 0.1268 - val_accuracy: 1.0000 - val_loss: 0.1186\n",
      "Epoch 8/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.1243 - val_accuracy: 1.0000 - val_loss: 0.1183\n",
      "Epoch 9/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9978 - loss: 0.1210 - val_accuracy: 1.0000 - val_loss: 0.1178\n",
      "Epoch 10/10\n",
      "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9986 - loss: 0.1194 - val_accuracy: 1.0000 - val_loss: 0.1176\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.1175  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌊 FloodNet Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"sampled_flood_data.csv\")\n",
    "\n",
    "print(\"✅ Enhanced dataset size:\", len(data), \n",
    "      \"| Flood:\", data['flood_binary'].sum(), \n",
    "      \"| No Flood:\", (data['flood_binary'] == 0).sum())\n",
    "\n",
    "X = data.drop(\"flood_binary\", axis=1).astype(\"float32\")\n",
    "y = data[\"flood_binary\"].astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- Custom Layers ---\n",
    "@register_keras_serializable()\n",
    "def rainfall_proximity_penalty(inputs):\n",
    "    rainfall = inputs[:, 0]\n",
    "    distance = inputs[:, 4]\n",
    "    proximity_score = tf.sigmoid((150 - distance) * 0.04)\n",
    "    rainfall_score = tf.sigmoid((rainfall - 90) * 0.3)\n",
    "    return (rainfall_score * proximity_score)[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def flood_risk_booster(inputs):\n",
    "    slope = inputs[:, 3]\n",
    "    rainfall = inputs[:, 0]\n",
    "    slope_boost = tf.sigmoid((slope - 2.0) * 1.5)\n",
    "    rain_boost = tf.sigmoid((rainfall - 60) * 0.25)\n",
    "    return (1.0 + 0.25 * slope_boost * rain_boost)[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def flood_suppression_mask(inputs):\n",
    "    elevation = inputs[:, 2]\n",
    "    rainfall = inputs[:, 0]\n",
    "    flatness = tf.sigmoid((elevation - 9.0) * 0.6)\n",
    "    dryness = tf.sigmoid((20.0 - rainfall) * 0.2)\n",
    "    return (1.0 - 0.3 * flatness * dryness)[:, None]\n",
    "\n",
    "class PrintValidationStats(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        preds = self.model.predict(self.validation_data[0])\n",
    "        print(f\"Epoch {epoch+1} raw preds (first 5):\", preds[:5].flatten())\n",
    "\n",
    "# Then pass it into `callbacks=[early_stop, PrintValidationStats()]`\n",
    "\n",
    "# --- Model Architecture (FireNet Style) ---\n",
    "input_layer = layers.Input(shape=(5,))\n",
    "x = layers.BatchNormalization()(input_layer)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "logits = layers.Dense(1)(x)\n",
    "\n",
    "penalty = layers.Lambda(rainfall_proximity_penalty, output_shape=(1,))(input_layer)\n",
    "booster = layers.Lambda(flood_risk_booster, output_shape=(1,))(input_layer)\n",
    "suppressor = layers.Lambda(flood_suppression_mask, output_shape=(1,))(input_layer)\n",
    "\n",
    "modulated_logits = layers.Add()([\n",
    "    logits,\n",
    "    layers.Multiply()([penalty, booster, suppressor])\n",
    "])\n",
    "adjusted_output = layers.Activation(\"sigmoid\")(modulated_logits)\n",
    "\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=adjusted_output)\n",
    "model.compile(optimizer=\"adam\", loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.05)\n",
    ", metrics=[\"accuracy\"])\n",
    "\n",
    "# --- Train & Evaluate ---\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=8, callbacks=[early_stop])\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"🌊 FloodNet Accuracy: {acc:.4f}\")\n",
    "model.save(\"models/FV-FloodNet.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
