{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aab37af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FlashFlood dataset rebuilt: (3400, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_flash_samples(label, n, center):\n",
    "    \"\"\"Generates synthetic flash flood data centered on specific conditions.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"rainfall_intensity\":   np.random.normal(loc=center[\"rain\"], scale=12, size=n),\n",
    "        \"slope\":                np.random.normal(loc=center[\"slope\"], scale=3.5, size=n),\n",
    "        \"drainage_density\":     np.random.normal(loc=center[\"drainage\"], scale=0.6, size=n),\n",
    "        \"soil_saturation\":      np.random.normal(loc=center[\"saturation\"], scale=0.1, size=n),\n",
    "        \"convergence_index\":  np.random.normal(loc=center[\"convergence\"], scale=0.1, size=n),\n",
    "        \"flash_binary\":         label\n",
    "    })\n",
    "\n",
    "# --- Core Archetypes ---\n",
    "flood_mountain_torrential = generate_flash_samples(1, 1000, {\n",
    "    \"rain\": 100, \"slope\": 20, \"drainage\": 1.5, \"saturation\": 0.9, \"convergence\": 0.8\n",
    "})\n",
    "\n",
    "flood_valley_choke = generate_flash_samples(1, 600, {\n",
    "    \"rain\": 80, \"slope\": 10, \"drainage\": 2.0, \"saturation\": 0.7, \"convergence\": 0.95\n",
    "})\n",
    "\n",
    "no_flood_flat_absorbent = generate_flash_samples(0, 800, {\n",
    "    \"rain\": 50, \"slope\": 5, \"drainage\": 4.5, \"saturation\": 0.4, \"convergence\": 0.4\n",
    "})\n",
    "\n",
    "no_flood_urban_controlled = generate_flash_samples(0, 700, {\n",
    "    \"rain\": 65, \"slope\": 8, \"drainage\": 4.0, \"saturation\": 0.5, \"convergence\": 0.5\n",
    "})\n",
    "\n",
    "contrast_mix = generate_flash_samples(1, 300, {\n",
    "    \"rain\": 90, \"slope\": 15, \"drainage\": 2.8, \"saturation\": 0.8, \"convergence\": 0.65\n",
    "})\n",
    "\n",
    "# --- Final Assembly ---\n",
    "data = pd.concat([\n",
    "    flood_mountain_torrential, flood_valley_choke, contrast_mix,\n",
    "    no_flood_flat_absorbent, no_flood_urban_controlled\n",
    "])\n",
    "data = data.clip(lower=0, upper=None)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data[\"rainfall_intensity\"] *= 1.5  # accentuate the rainfall signal\n",
    "\n",
    "data.to_csv(\"dataset/flash_flood_data.csv\", index=False)\n",
    "print(\"✅ FlashFlood dataset rebuilt:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57806de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84df52e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset size: 3400 | Flood: 1900 | No Flood: 1500\n",
      "WARNING:tensorflow:From c:\\Users\\meher\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.8224 - loss: 0.9121 - val_accuracy: 0.9769 - val_loss: 0.1747\n",
      "Epoch 2/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9718 - loss: 0.1885 - val_accuracy: 1.0000 - val_loss: 0.1408\n",
      "Epoch 3/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9766 - loss: 0.1815 - val_accuracy: 0.9979 - val_loss: 0.1506\n",
      "Epoch 4/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9545 - loss: 0.2116 - val_accuracy: 1.0000 - val_loss: 0.1354\n",
      "Epoch 5/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.9641 - loss: 0.1762 - val_accuracy: 1.0000 - val_loss: 0.1344\n",
      "Epoch 6/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9802 - loss: 0.1699 - val_accuracy: 1.0000 - val_loss: 0.1305\n",
      "Epoch 7/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.1761 - val_accuracy: 1.0000 - val_loss: 0.1304\n",
      "Epoch 8/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9769 - loss: 0.1654 - val_accuracy: 1.0000 - val_loss: 0.1289\n",
      "Epoch 9/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9773 - loss: 0.1680 - val_accuracy: 1.0000 - val_loss: 0.1296\n",
      "Epoch 10/10\n",
      "\u001b[1m238/238\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.1680 - val_accuracy: 1.0000 - val_loss: 0.1343\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.1277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ FlashFloodNet Accuracy: 0.9971\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(\"dataset/flash_flood_data.csv\")\n",
    "print(\"✅ Dataset size:\", len(data),\n",
    "      \"| Flood:\", data[\"flash_binary\"].sum(),\n",
    "      \"| No Flood:\", (data[\"flash_binary\"] == 0).sum())\n",
    "\n",
    "X = data.drop(\"flash_binary\", axis=1).astype(\"float32\")\n",
    "y = data[\"flash_binary\"].astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "@register_keras_serializable()\n",
    "def intensity_slope_amplifier(inputs):\n",
    "    rain_intensity = inputs[:, 0]\n",
    "    slope = inputs[:, 1]\n",
    "    runoff_boost = tf.sigmoid((rain_intensity - 75) * 0.08)\n",
    "    slope_boost = tf.sigmoid((slope - 10) * 0.05)\n",
    "    return (1.0 + 0.35 * runoff_boost * slope_boost)[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def drainage_penalty(inputs):\n",
    "    drainage = inputs[:, 2]\n",
    "    return (1.0 - 0.4 * tf.sigmoid((drainage - 3.5) * 2))[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def convergence_suppressor(inputs):\n",
    "    convergence = inputs[:, 4]\n",
    "    return (1.0 + 0.3 * tf.sigmoid((convergence - 0.5) * 8))[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def clip_modulation(x):\n",
    "    return tf.clip_by_value(x, 0.7, 1.3)\n",
    "\n",
    "input_layer = layers.Input(shape=(5,))\n",
    "rain_input = layers.Lambda(lambda x: x[:, 0:1])(input_layer)\n",
    "rain_branch = layers.Dense(8, activation=\"relu\")(rain_input)\n",
    "\n",
    "x = layers.BatchNormalization()(input_layer)\n",
    "x1 = layers.Dense(128, activation=\"relu\")(x)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "x3 = layers.Dense(64, activation=\"relu\")(x2)\n",
    "residual = layers.Add()([x3, x2])\n",
    "combined = layers.Concatenate()([residual, rain_branch])\n",
    "logits = layers.Dense(1)(combined)\n",
    "\n",
    "amplifier   = layers.Lambda(intensity_slope_amplifier)(input_layer)\n",
    "penalty     = layers.Lambda(drainage_penalty)(input_layer)\n",
    "suppression = layers.Lambda(convergence_suppressor)(input_layer)\n",
    "mod_strength = layers.Multiply()([amplifier, penalty, suppression])\n",
    "mod_strength = layers.Lambda(clip_modulation)(mod_strength)\n",
    "\n",
    "modulated_logits = layers.Add()([logits, mod_strength])\n",
    "adjusted_output  = layers.Activation(\"sigmoid\")(modulated_logits)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=adjusted_output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.05),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=8, callbacks=[early_stop])\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"⚡ FlashFloodNet Accuracy: {acc:.4f}\")\n",
    "model.save(\"models/FlashFloodNet.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
