{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a2906d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HurricaneNet dataset generated: (4100, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_hurricane_samples(label, n, center):\n",
    "    return pd.DataFrame({\n",
    "        \"sea_surface_temperature\":    np.random.normal(center[\"sst\"], 0.8, n),\n",
    "        \"ocean_heat_content\":         np.random.normal(center[\"ohc\"], 10, n),\n",
    "        \"mid_level_humidity\":         np.random.normal(center[\"humidity\"], 8, n),\n",
    "        \"vertical_wind_shear\":        np.random.normal(center[\"shear\"], 3, n),\n",
    "        \"potential_vorticity\":        np.random.normal(center[\"vort\"], 0.4, n),\n",
    "        \"hurricane_binary\":           label\n",
    "    })\n",
    "\n",
    "# --- Archetypes ---\n",
    "atlantic_warm_blob = generate_hurricane_samples(1, 1200, {\n",
    "    \"sst\": 29.5, \"ohc\": 110, \"humidity\": 75,\n",
    "    \"shear\": 4, \"vort\": 1.8\n",
    "})\n",
    "\n",
    "saharan_suppressed = generate_hurricane_samples(0, 900, {\n",
    "    \"sst\": 27, \"ohc\": 70, \"humidity\": 40,\n",
    "    \"shear\": 18, \"vort\": 0.5\n",
    "})\n",
    "\n",
    "pre_spin_wave = generate_hurricane_samples(1, 800, {\n",
    "    \"sst\": 28.5, \"ohc\": 90, \"humidity\": 65,\n",
    "    \"shear\": 10, \"vort\": 1.2\n",
    "})\n",
    "\n",
    "dead_zone = generate_hurricane_samples(0, 700, {\n",
    "    \"sst\": 26, \"ohc\": 50, \"humidity\": 35,\n",
    "    \"shear\": 20, \"vort\": 0.4\n",
    "})\n",
    "\n",
    "mixed_noise = generate_hurricane_samples(1, 500, {\n",
    "    \"sst\": 29, \"ohc\": 95, \"humidity\": 60,\n",
    "    \"shear\": 9, \"vort\": 1.1\n",
    "})\n",
    "\n",
    "# --- Final Assembly ---\n",
    "data = pd.concat([\n",
    "    atlantic_warm_blob,\n",
    "    saharan_suppressed,\n",
    "    pre_spin_wave,\n",
    "    dead_zone,\n",
    "    mixed_noise\n",
    "])\n",
    "data = data.clip(lower=0).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data.to_csv(\"dataset/hurricane_data.csv\", index=False)\n",
    "\n",
    "print(\"HurricaneNet dataset generated:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa366ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\meher\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.7297 - loss: 0.4759 - val_accuracy: 0.6237 - val_loss: 1.3863\n",
      "Epoch 2/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9804 - loss: 0.1697 - val_accuracy: 0.9791 - val_loss: 0.1718\n",
      "Epoch 3/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9751 - loss: 0.1698 - val_accuracy: 0.9930 - val_loss: 0.1353\n",
      "Epoch 4/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9882 - loss: 0.1454 - val_accuracy: 0.9948 - val_loss: 0.1312\n",
      "Epoch 5/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9794 - loss: 0.1596 - val_accuracy: 0.9983 - val_loss: 0.1302\n",
      "Epoch 6/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9851 - loss: 0.1540 - val_accuracy: 0.9965 - val_loss: 0.1321\n",
      "Epoch 7/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9699 - loss: 0.1790 - val_accuracy: 0.9983 - val_loss: 0.1287\n",
      "Epoch 8/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9831 - loss: 0.1570 - val_accuracy: 0.9948 - val_loss: 0.1302\n",
      "Epoch 9/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9854 - loss: 0.1495 - val_accuracy: 0.9965 - val_loss: 0.1300\n",
      "Epoch 10/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9783 - loss: 0.1602 - val_accuracy: 0.9948 - val_loss: 0.1338\n",
      "Epoch 11/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9834 - loss: 0.1572 - val_accuracy: 0.9983 - val_loss: 0.1288\n",
      "Epoch 12/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9860 - loss: 0.1494 - val_accuracy: 0.9948 - val_loss: 0.1280\n",
      "Epoch 13/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9662 - loss: 0.1840 - val_accuracy: 0.9965 - val_loss: 0.1290\n",
      "Epoch 14/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9617 - loss: 0.1882 - val_accuracy: 0.9948 - val_loss: 0.1307\n",
      "Epoch 15/15\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.9823 - loss: 0.1511 - val_accuracy: 0.9983 - val_loss: 0.1284\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HurricaneNet Accuracy: 0.9967\n",
      "WARNING:tensorflow:From c:\\Users\\meher\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf2onnx\\tf_loader.py:72: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\meher\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf2onnx\\tf_loader.py:72: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "ERROR:tf2onnx.tfonnx:rewriter <function rewrite_constant_fold at 0x00000201D1353E20>: exception `np.cast` was removed in the NumPy 2.0 release. Use `np.asarray(arr, dtype=dtype)` instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "\n",
    "data = pd.read_csv(\"dataset/hurricane_data.csv\")\n",
    "X = data.drop(\"hurricane_binary\", axis=1).astype(\"float32\")\n",
    "y = data[\"hurricane_binary\"].astype(\"float32\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "@register_keras_serializable()\n",
    "class SSTAmplifier(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=28.0, scale=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.scale = scale\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sst = inputs[:, 0]\n",
    "        factor = tf.sigmoid((sst - self.threshold) * self.scale)\n",
    "        mod = 1.0 + 0.3 * factor\n",
    "        return tf.expand_dims(mod, -1)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ShearSuppressor(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=14.0, scale=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.scale = scale\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shear = inputs[:, 3]\n",
    "        suppress = tf.sigmoid((self.threshold - shear) * self.scale)\n",
    "        mod = 1.0 - 0.25 * suppress\n",
    "        return tf.expand_dims(mod, -1)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class VorticityActivator(tf.keras.layers.Layer):\n",
    "    def __init__(self, threshold=1.2, scale=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.scale = scale\n",
    "\n",
    "    def call(self, inputs):\n",
    "        vort = inputs[:, 4]\n",
    "        activate = tf.sigmoid((vort - self.threshold) * self.scale)\n",
    "        mod = 1.0 + 0.2 * activate\n",
    "        return tf.expand_dims(mod, -1)\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ModulationMixer(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        sst_mod, shear_mod, vort_mod = inputs\n",
    "        product = sst_mod * shear_mod * vort_mod\n",
    "        smooth = 1.0 + 0.25 * tf.tanh(product - 1.0)\n",
    "        return smooth\n",
    "\n",
    "# --- Input Layer ---\n",
    "input_layer = layers.Input(shape=(5,), name=\"hurricane_inputs\")\n",
    "\n",
    "x = layers.BatchNormalization()(input_layer)\n",
    "x1 = layers.Dense(64, activation=\"relu\")(x)\n",
    "x2 = layers.Dense(32, activation=\"relu\")(x1)\n",
    "x3 = layers.Dense(16, activation=\"relu\")(x2)\n",
    "base_logits = layers.Dense(1)(x3)\n",
    "\n",
    "sst_mod    = SSTAmplifier()(input_layer)\n",
    "shear_mod  = ShearSuppressor()(input_layer)\n",
    "vort_mod   = VorticityActivator()(input_layer)\n",
    "mod_strength = ModulationMixer()([sst_mod, shear_mod, vort_mod])\n",
    "\n",
    "combined_logits = layers.Add()([base_logits, mod_strength])\n",
    "final_output = layers.Activation(\"sigmoid\")(combined_logits)\n",
    "model = models.Model(inputs=input_layer, outputs=final_output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.05),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=15, batch_size=16, callbacks=[early_stop])\n",
    "\n",
    "# --- Evaluate & Save ---\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"HurricaneNet Accuracy: {acc:.4f}\")\n",
    "model.save(\"models/HurricaneNet.h5\")\n",
    "import tf2onnx\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model)\n",
    "with open(f\"models/ONNX/HurricaneNet.onnx\", \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18766d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌀 HurricaneNet Scenario Evaluation:\n",
      "\n",
      "🟠 Atlantic Warm Blob\n",
      "  ➤ Features      : [29.7, 115, 78, 3.5, 2.0]\n",
      "  ➤ Predicted     : Hurricane (confidence: 0.98)\n",
      "  ➤ Expected      : Hurricane\n",
      "\n",
      "🟤 Saharan Air Layer Suppression\n",
      "  ➤ Features      : [27.1, 68, 42, 19, 0.6]\n",
      "  ➤ Predicted     : No Hurricane (confidence: 0.04)\n",
      "  ➤ Expected      : No Hurricane\n",
      "\n",
      "🌊 Tropical Wave Pre-Spin\n",
      "  ➤ Features      : [28.6, 93, 67, 9.5, 1.4]\n",
      "  ➤ Predicted     : Hurricane (confidence: 0.97)\n",
      "  ➤ Expected      : Hurricane\n",
      "\n",
      "🌫️ Oceanic Dead Zone\n",
      "  ➤ Features      : [26.3, 48, 38, 21, 0.5]\n",
      "  ➤ Predicted     : No Hurricane (confidence: 0.02)\n",
      "  ➤ Expected      : No Hurricane\n",
      "\n",
      "🌬️ Shear-Cut Anomaly\n",
      "  ➤ Features      : [29.0, 98, 62, 17, 1.2]\n",
      "  ➤ Predicted     : Hurricane (confidence: 0.96)\n",
      "  ➤ Expected      : Possibly Hurricane\n",
      "\n",
      "🌡️ Hot SST / Low Vorticity\n",
      "  ➤ Features      : [30.1, 120, 75, 6, 0.3]\n",
      "  ➤ Predicted     : Hurricane (confidence: 0.99)\n",
      "  ➤ Expected      : Edge Case\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scenarios = [\n",
    "    {\n",
    "        \"label\": \"🟠 Atlantic Warm Blob\",\n",
    "        \"features\": [29.7, 115, 78, 3.5, 2.0],\n",
    "        \"expected\": \"Hurricane\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"🟤 Saharan Air Layer Suppression\",\n",
    "        \"features\": [27.1, 68, 42, 19, 0.6],\n",
    "        \"expected\": \"No Hurricane\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"🌊 Tropical Wave Pre-Spin\",\n",
    "        \"features\": [28.6, 93, 67, 9.5, 1.4],\n",
    "        \"expected\": \"Hurricane\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"🌫️ Oceanic Dead Zone\",\n",
    "        \"features\": [26.3, 48, 38, 21, 0.5],\n",
    "        \"expected\": \"No Hurricane\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"🌬️ Shear-Cut Anomaly\",\n",
    "        \"features\": [29.0, 98, 62, 17, 1.2],\n",
    "        \"expected\": \"Possibly Hurricane\"\n",
    "    },\n",
    "    {\n",
    "        \"label\": \"🌡️ Hot SST / Low Vorticity\",\n",
    "        \"features\": [30.1, 120, 75, 6, 0.3],\n",
    "        \"expected\": \"Edge Case\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Run predictions ---\n",
    "print(\"\\n🌀 HurricaneNet Scenario Evaluation:\\n\")\n",
    "for case in scenarios:\n",
    "    x = np.array(case[\"features\"], dtype=\"float32\").reshape(1, -1)\n",
    "    pred = model(x).numpy()[0][0]\n",
    "    verdict = (\n",
    "        \"Hurricane\" if pred > 0.55 else\n",
    "        \"Possibly Hurricane\" if 0.4 < pred <= 0.55 else\n",
    "        \"No Hurricane\"\n",
    "    )\n",
    "\n",
    "    print(f\"{case['label']}\")\n",
    "    print(f\"  ➤ Features      : {case['features']}\")\n",
    "    print(f\"  ➤ Predicted     : {verdict} (confidence: {pred:.2f})\")\n",
    "    print(f\"  ➤ Expected      : {case['expected']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ffea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
