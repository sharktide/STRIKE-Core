{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aecf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pluvial dataset rebuilt: (3300, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_samples(label, n, center):\n",
    "    \"\"\"Generates synthetic pluvial data centered around certain conditions.\"\"\"\n",
    "    return pd.DataFrame({\n",
    "        \"rainfall_intensity\": np.random.normal(loc=center[\"rainfall\"], scale=12, size=n),\n",
    "        \"impervious_ratio\":   np.random.normal(loc=center[\"impervious\"], scale=0.08, size=n),\n",
    "        \"drainage_density\":   np.random.normal(loc=center[\"drainage\"], scale=0.4, size=n),\n",
    "        \"urbanization_index\": np.random.normal(loc=center[\"urban\"], scale=0.08, size=n),\n",
    "        \"convergence_index\":  np.random.normal(loc=center[\"convergence\"], scale=0.1, size=n),\n",
    "        \"pluvial_binary\":     label\n",
    "    })\n",
    "\n",
    "# --- Core Archetypes ---\n",
    "flood_urban = generate_samples(1, 1000, {\n",
    "    \"rainfall\": 45, \"impervious\": 0.8, \"drainage\": 1.5, \"urban\": 0.85, \"convergence\": 0.75\n",
    "})\n",
    "\n",
    "flood_rural_rain = generate_samples(1, 500, {\n",
    "    \"rainfall\": 90, \"impervious\": 0.3, \"drainage\": 2.5, \"urban\": 0.35, \"convergence\": 0.85\n",
    "})\n",
    "\n",
    "no_flood_urban_dry = generate_samples(0, 700, {\n",
    "    \"rainfall\": 15, \"impervious\": 0.8, \"drainage\": 3.5, \"urban\": 0.8, \"convergence\": 0.6\n",
    "})\n",
    "\n",
    "no_flood_rural = generate_samples(0, 800, {\n",
    "    \"rainfall\": 35, \"impervious\": 0.25, \"drainage\": 4.2, \"urban\": 0.25, \"convergence\": 0.35\n",
    "})\n",
    "\n",
    "contrast_mix = generate_samples(1, 300, {\n",
    "    \"rainfall\": 60, \"impervious\": 0.5, \"drainage\": 2.8, \"urban\": 0.6, \"convergence\": 0.55\n",
    "})\n",
    "\n",
    "data = pd.concat([flood_urban, flood_rural_rain, contrast_mix, no_flood_urban_dry, no_flood_rural])\n",
    "data = data.clip(lower=0, upper=None)\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "data[\"rainfall_intensity\"] *= 2\n",
    "\n",
    "data.to_csv(\"dataset/pluvial_flood_data_balanced.csv\", index=False)\n",
    "print(\"âœ… Pluvial dataset rebuilt:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Pluvial dataset size: 3300 | Flood: 1800 | No Flood: 1500\n",
      "Epoch 1/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.8445 - loss: 0.3536 - val_accuracy: 0.9827 - val_loss: 0.1705\n",
      "Epoch 2/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9669 - loss: 0.1941 - val_accuracy: 0.9892 - val_loss: 0.1609\n",
      "Epoch 3/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9760 - loss: 0.1806 - val_accuracy: 0.9957 - val_loss: 0.1425\n",
      "Epoch 4/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.2045 - val_accuracy: 0.9935 - val_loss: 0.1437\n",
      "Epoch 5/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9676 - loss: 0.1955 - val_accuracy: 0.9957 - val_loss: 0.1472\n",
      "Epoch 6/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9795 - loss: 0.1773 - val_accuracy: 0.9935 - val_loss: 0.1483\n",
      "Epoch 7/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.1753 - val_accuracy: 0.9935 - val_loss: 0.1452\n",
      "Epoch 8/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9811 - loss: 0.1702 - val_accuracy: 0.9935 - val_loss: 0.1419\n",
      "Epoch 9/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9651 - loss: 0.1982 - val_accuracy: 0.9957 - val_loss: 0.1452\n",
      "Epoch 10/10\n",
      "\u001b[1m231/231\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9783 - loss: 0.1731 - val_accuracy: 0.9935 - val_loss: 0.1419\n",
      "\u001b[1m31/31\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9980 - loss: 0.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ§ï¸ PluvialNet Accuracy: 0.9960\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Load Pluvial Dataset ---\n",
    "data = pd.read_csv(\"dataset/pluvial_flood_data_balanced.csv\")\n",
    "\n",
    "print(\"âœ… Pluvial dataset size:\", len(data),\n",
    "      \"| Flood:\", data['pluvial_binary'].sum(),\n",
    "      \"| No Flood:\", (data['pluvial_binary'] == 0).sum())\n",
    "\n",
    "X = data.drop(\"pluvial_binary\", axis=1).astype(\"float32\")\n",
    "y = data[\"pluvial_binary\"].astype(\"float32\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "\n",
    "@register_keras_serializable()\n",
    "def surface_runoff_amplifier(inputs):\n",
    "    rain = inputs[:, 0]\n",
    "    impervious = inputs[:, 1]\n",
    "    rain_boost = tf.sigmoid((rain - 60) * 0.06)\n",
    "    impervious_boost = tf.sigmoid((impervious - 0.6) * 10)\n",
    "    return (1.0 + 0.3 * rain_boost * impervious_boost)[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def drainage_penalty(inputs):\n",
    "    dd = inputs[:, 2]\n",
    "    return (1.0 - 0.4 * tf.sigmoid((dd - 3.5) * 2))[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def convergence_suppressor(inputs):\n",
    "    ci = inputs[:, 4]\n",
    "    return (1.0 + 0.3 * tf.sigmoid((ci - 0.5) * 8))[:, None]\n",
    "\n",
    "@register_keras_serializable()\n",
    "def clip_modulation(x):\n",
    "    return tf.clip_by_value(x, 0.7, 1.3)\n",
    "\n",
    "input_layer = layers.Input(shape=(5,))\n",
    "\n",
    "rain_input = layers.Lambda(lambda x: x[:, 0:1])(input_layer)\n",
    "rain_branch = layers.Dense(8, activation=\"relu\")(rain_input)\n",
    "\n",
    "# --- Main trunk with residual connection ---\n",
    "x = layers.BatchNormalization()(input_layer)\n",
    "x1 = layers.Dense(128, activation=\"relu\")(x)\n",
    "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
    "x3 = layers.Dense(64, activation=\"relu\")(x2)\n",
    "\n",
    "residual = layers.Add()([x3, x2])  # â† residual connection\n",
    "combined = layers.Concatenate()([residual, rain_branch])\n",
    "logits = layers.Dense(1)(combined)\n",
    "\n",
    "\n",
    "amplifier    = layers.Lambda(surface_runoff_amplifier)(input_layer)\n",
    "penalty      = layers.Lambda(drainage_penalty)(input_layer)\n",
    "suppression  = layers.Lambda(convergence_suppressor)(input_layer)\n",
    "\n",
    "mod_strength = layers.Multiply()([amplifier, penalty, suppression])\n",
    "mod_strength = layers.Lambda(clip_modulation)(mod_strength)\n",
    "\n",
    "modulated_logits = layers.Add()([logits, mod_strength])\n",
    "adjusted_output  = layers.Activation(\"sigmoid\")(modulated_logits)\n",
    "\n",
    "model = models.Model(inputs=input_layer, outputs=adjusted_output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.05),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# --- Train & Evaluate ---\n",
    "early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=10, batch_size=8, callbacks=[early_stop])\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"ğŸŒ§ï¸ PluvialNet Accuracy: {acc:.4f}\")\n",
    "model.save(\"models/PV-FloodNet.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
